{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from GAIN_imputer_utility import xavier_init,MyDataset,preprocess,load_dataloader,Imputation_model,get_dataset_loaders,set_all_BN_layers_tracking_state,loss,impute_with_prediction,check_and_fill_nan\n",
    "from sklearn.impute import SimpleImputer\n",
    "from MNAR.missing_process.block_rules import *\n",
    "os.chdir(\"BatchNorm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_type = \"quantile\"\n",
    "\n",
    "  # set it to True to use GPU and False to use CPU\n",
    "use_BN = False\n",
    "states = [True,True]\n",
    "\n",
    "dataset_file = 'california'\n",
    "\n",
    "missing_rule = [\"Q1_complete\",\"Q1_partial\",\"Q2_complete\",\"Q2_partial\",\"Q3_complete\",\"Q3_partial\",\"Q4_complete\",\"Q4_partial\",\n",
    "\"Q1_Q2_complete\",\"Q1_Q2_partial\",\"Q1_Q3_complete\",\"Q1_Q3_partial\",\"Q1_Q4_complete\",\"Q1_Q4_partial\",\"Q2_Q3_complete\",\"Q2_Q3_partial\",\n",
    "\"Q2_Q4_complete\",\"Q2_Q4_partial\",\"Q3_Q4_complete\",\"Q3_Q4_partial\"]\n",
    "\n",
    "missing_rule = [\"C0_lower\",\"C0_upper\",\"C0_double\",\"C1_lower\",\"C1_upper\",\"C1_double\", \n",
    "                \"C2_lower\",\"C2_upper\",\"C2_double\", \"C3_lower\",\"C3_upper\", \"C3_double\",\n",
    "                \"C4_lower\",\"C4_upper\",\"C4_double\",\"C5_lower\",\"C5_upper\",\"C5_double\",\n",
    "                \"C6_lower\",\"C6_upper\",\"C6_double\",\"C7_lower\",\"C7_upper\",\"C7_double\",\n",
    "]\n",
    "\n",
    "missing_rule = [\"C0_lower\"]\n",
    "\n",
    "missing_type = \"BN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(truth, mask, data,imputer):\n",
    "\n",
    "    generated = imputer(data, mask)\n",
    "\n",
    "    #print(generated[1,:])\n",
    "\n",
    "    return  torch.mean(((1 - mask) * truth - (1 - mask) * generated) ** 2) / torch.mean(1 - mask), generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imputation_model(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim1, hidden_dim2,use_BN):\n",
    "        super(Imputation_model, self).__init__()\n",
    "    \n",
    "        self.G_W1 = nn.Parameter(torch.tensor(xavier_init([dim * 2, hidden_dim1]), dtype=torch.float32), requires_grad=True)\n",
    "        self.G_b1 = nn.Parameter(torch.zeros(hidden_dim1, dtype=torch.float32), requires_grad=True)\n",
    "        self.G_bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "\n",
    "        self.G_W2 = nn.Parameter(torch.tensor(xavier_init([hidden_dim1, hidden_dim2]), dtype=torch.float32), requires_grad=True)\n",
    "        self.G_b2 = nn.Parameter(torch.zeros(hidden_dim2, dtype=torch.float32), requires_grad=True)\n",
    "        self.G_bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "\n",
    "        self.G_W3 = nn.Parameter(torch.tensor(xavier_init([hidden_dim2, dim]), dtype=torch.float32), requires_grad=True)\n",
    "        self.G_b3 = nn.Parameter(torch.zeros(dim, dtype=torch.float32), requires_grad=True)\n",
    "\n",
    "        self.use_BN = use_BN\n",
    "        self.batch_mean1 = None\n",
    "        self.batch_var1 = None\n",
    "\n",
    "        self.batch_mean2 = None\n",
    "        self.batch_var2 = None\n",
    "\n",
    "    def forward(self, data, mask):\n",
    "        inputs = torch.cat(dim=1, tensors=[data, mask])  # Mask + Data Concatenate\n",
    "        inputs = inputs.float()  \n",
    "        G_h1 = F.relu(torch.matmul(inputs, self.G_W1.float()) + self.G_b1.float())\n",
    "        if self.use_BN:\n",
    "            G_h1 = self.G_bn1(G_h1)  # Batch Normalization\n",
    "        G_h2 = F.relu(torch.matmul(G_h1, self.G_W2.float()) + self.G_b2.float())\n",
    "        if self.use_BN:\n",
    "            G_h2 = self.G_bn2(G_h2)  # Batch Normalization\n",
    "        G_prob = torch.sigmoid(torch.matmul(G_h2, self.G_W3.float()) + self.G_b3.float())  # [0,1] normalized Output\n",
    "\n",
    "        if self.use_BN:\n",
    "            self.batch_mean1 = self.G_bn1.running_mean\n",
    "            self.batch_var1 = self.G_bn1.running_var\n",
    "            self.batch_mean2 = self.G_bn2.running_mean\n",
    "            self.batch_var2 = self.G_bn2.running_var\n",
    "\n",
    "        return G_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imputation_model(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim1, hidden_dim2, use_BN):\n",
    "        super(Imputation_model, self).__init__()\n",
    "\n",
    "        self.G_W1 = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty([dim * 2, hidden_dim1])), requires_grad=True)\n",
    "        self.G_b1 = nn.Parameter(torch.zeros(hidden_dim1), requires_grad=True)\n",
    "\n",
    "        self.G_W2 = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty([hidden_dim1, hidden_dim2])), requires_grad=True)\n",
    "        self.G_b2 = nn.Parameter(torch.zeros(hidden_dim2), requires_grad=True)\n",
    "\n",
    "        self.G_W3 = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty([hidden_dim2, dim])), requires_grad=True)\n",
    "        self.G_b3 = nn.Parameter(torch.zeros(dim), requires_grad=True)\n",
    "\n",
    "        self.use_BN = use_BN\n",
    "\n",
    "        if self.use_BN:\n",
    "            self.batch_norm_layers = nn.ModuleList([nn.BatchNorm1d(hidden_dim1), nn.BatchNorm1d(hidden_dim2)])\n",
    "\n",
    "    def forward(self, data, mask):\n",
    "        inputs = torch.cat([data, mask], dim=1).float()\n",
    "        G_h1 = nn.ReLU()(torch.matmul(inputs, self.G_W1) + self.G_b1)\n",
    "\n",
    "        if self.use_BN:\n",
    "            G_h1 = self.batch_norm_layers[0](G_h1)\n",
    "\n",
    "        G_h2 = nn.ReLU()(torch.matmul(G_h1, self.G_W2) + self.G_b2)\n",
    "\n",
    "        if self.use_BN:\n",
    "            G_h2 = self.batch_norm_layers[1](G_h2)\n",
    "\n",
    "        G_prob = torch.sigmoid(torch.matmul(G_h2, self.G_W3) + self.G_b3)\n",
    "\n",
    "        return G_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10  # Number of epochs to wait for improvement\n",
    "best_validation_loss = float('inf')\n",
    "best_model_state = None\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dataset_file,missing_rule, use_BN):\n",
    "    \n",
    "   \n",
    "\n",
    "    Imputer_RMSE = []\n",
    "    baseline_RMSE = []\n",
    "    \n",
    "    for rule_name in missing_rule:\n",
    "        print(dataset_file,rule_name,use_BN,states)\n",
    "        trainX, testX, train_Mask, test_Mask, train_input, test_input, No, Dim,train_H, test_H,Xval_org, Xval_org_mask, val_input, val_H = load_dataloader(dataset_file,missing_type, rule_name)\n",
    "\n",
    "    \n",
    "        # train_dataset, test_dataset = MyDataset(trainX, train_Mask,train_input,train_H), MyDataset(testX, test_Mask, test_input,test_H)\n",
    "\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        imputer = Imputation_model(Dim, Dim, Dim, use_BN)\n",
    "        #imputer = Simple_imputer(Dim)\n",
    "        optimizer = torch.optim.Adam(params=imputer.parameters())\n",
    "\n",
    "        train_loader , test_loader, test_loader = get_dataset_loaders(trainX, train_Mask,train_input,testX, test_Mask,test_input,train_H, test_H,Xval_org, Xval_org_mask, val_input, val_H )\n",
    "\n",
    "\n",
    "        for it in tqdm(range(epoch)):\n",
    "            imputer.train()\n",
    "            total_loss = 0\n",
    "            batch_no = 0\n",
    "            for truth_X, mask, data_X,x_hat in train_loader:\n",
    "                batch_no += 1\n",
    "\n",
    "                # print(\"======Batch {} Start======\".format(batch_no))\n",
    "                # print('Running first pass:')\n",
    "\n",
    "                set_all_BN_layers_tracking_state(imputer,True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                Imputer_loss = loss(truth=truth_X, mask=mask, data=data_X, imputer = imputer)[0]\n",
    "  \n",
    "                total_loss += Imputer_loss\n",
    "                Imputer_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print('1st BatchNorm Mean: {:.4} Var:{:.4}'.format(torch.mean(imputer.batch_mean1), torch.mean(imputer.batch_var1)))\n",
    "                # print('2nt BatchNorm Mean: {:.4} Var:{:.4}'.format(torch.mean(imputer.batch_mean2), torch.mean(imputer.batch_var2)), end='\\n\\n')\n",
    "\n",
    "                # print('Running second pass:')\n",
    "\n",
    "                set_all_BN_layers_tracking_state(imputer,True)\n",
    "\n",
    "                # prediction = loss(truth=truth_X, mask=mask, data=data_X,imputer = imputer)[1]\n",
    "\n",
    "                # imputed_data = impute_with_prediction(truth_X, mask, prediction)\n",
    "\n",
    "                # _ = imputer(imputed_data, mask)\n",
    "\n",
    "\n",
    "                # print('1st BatchNorm Mean: {:.4} Var:{:.4}'.format(torch.mean(imputer.batch_mean1), torch.mean(imputer.batch_var1)))\n",
    "                # print('2nt BatchNorm Mean: {:.4} Var:{:.4}'.format(torch.mean(imputer.batch_mean2), torch.mean(imputer.batch_var2)), end='\\n\\n')\n",
    "\n",
    "\n",
    "                # print(\"======Batch {} End======\\n\\n\".format(batch_no))\n",
    "\n",
    "\n",
    "            print('Iter: {}'.format(it), end='\\t')\n",
    "            print(total_loss.item(),batch_no)\n",
    "            print('Train_loss: {:.4}'.format(np.sqrt(total_loss.item()/batch_no)))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "\n",
    "        with torch.no_grad():\n",
    "            imputer.eval()\n",
    "            RMSE_total = []\n",
    "            for truth_X, mask, data_X, x_hat in test_loader:\n",
    "\n",
    "                RMSE, prediction =  loss(truth=truth_X, mask=mask, data=data_X,imputer = imputer)\n",
    "                imputed_data = impute_with_prediction(truth_X, mask, prediction)\n",
    "                RMSE_total.append(RMSE)\n",
    "\n",
    "\n",
    "        RMSE_tensor = torch.tensor(RMSE_total)\n",
    "        rmse_final = torch.mean(RMSE_tensor)\n",
    "\n",
    "        Imputer_RMSE.append(round(rmse_final.item(),5))\n",
    "\n",
    "        print('Final Test RMSE: {:.4f}'.format(rmse_final.item()))\n",
    "\n",
    "\n",
    "\n",
    "    result = pd.DataFrame({\"Missing_Rule\":[rule_name for rule_name in missing_rule],\"Imputer RMSE\":Imputer_RMSE,\"Baseline RMSE\":baseline_RMSE})\n",
    "    print(result)\n",
    "    #result.to_csv(\"results/GAIN_imputer/{}_0pass.csv\".format(dataset_file),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "def train_with_early_stopping(imputer, train_loader, test_loader, epoch, patience):\n",
    "    # Define early stopping parameters\n",
    "    best_validation_loss = float('inf')  # Initialize with a high value\n",
    "    best_model_state = None\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=imputer.parameters())\n",
    "\n",
    "    for it in tqdm(range(epoch)):\n",
    "        imputer.train()\n",
    "        total_loss = 0\n",
    "        batch_no = 0\n",
    "\n",
    "        for truth_X, mask, data_X, x_hat in train_loader:\n",
    "            batch_no += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            Imputer_loss = loss(truth=truth_X, mask=mask, data=data_X, imputer=imputer)[0]\n",
    "            total_loss += Imputer_loss.item()\n",
    "            Imputer_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = np.sqrt(total_loss / batch_no)\n",
    "\n",
    "        # Validation step\n",
    "        imputer.eval()\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            val_batch_no = 0\n",
    "\n",
    "            for truth_X_val, mask_val, data_X_val, x_hat_val in test_loader:\n",
    "                val_batch_no += 1\n",
    "\n",
    "                val_loss = loss(truth=truth_X_val, mask=mask_val, data=data_X_val, imputer=imputer)[0]\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = np.sqrt(total_val_loss / val_batch_no)\n",
    "\n",
    "        # Check for improvement in validation loss\n",
    "        if avg_val_loss < best_validation_loss:\n",
    "            best_validation_loss = avg_val_loss\n",
    "            best_model_state = imputer.state_dict()\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        # Print current epoch's training and validation loss\n",
    "        print('Epoch: {}'.format(it), end='\\t')\n",
    "        print('Train_loss: {:.4}'.format(avg_train_loss), end='\\t')\n",
    "        print('Val_loss: {:.4}'.format(avg_val_loss), end='\\n')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping! No improvement in validation loss for {} epochs.\".format(patience))\n",
    "            break\n",
    "\n",
    "    # Load the best model state after training loop completes\n",
    "    imputer.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run2(dataset_file,missing_rule, use_BN):\n",
    "    \n",
    "   \n",
    "\n",
    "    Imputer_RMSE = []\n",
    "    baseline_RMSE = []\n",
    "    \n",
    "    for rule_name in missing_rule:\n",
    "        print(dataset_file,rule_name,use_BN,states)\n",
    "        trainX, testX, train_Mask, test_Mask, train_input, test_input, No, Dim,train_H, test_H,Xval_org, Xval_org_mask, val_input, val_H = load_dataloader(dataset_file,missing_type, rule_name)\n",
    "\n",
    "    \n",
    "        # train_dataset, test_dataset = MyDataset(trainX, train_Mask,train_input,train_H), MyDataset(testX, test_Mask, test_input,test_H)\n",
    "\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        imputer = Imputation_model(Dim, Dim, Dim, use_BN)\n",
    "        #imputer = Simple_imputer(Dim)\n",
    "        optimizer = torch.optim.Adam(params=imputer.parameters())\n",
    "\n",
    "        train_loader , test_loader, test_loader = get_dataset_loaders(trainX, train_Mask,train_input,testX, test_Mask,test_input,train_H, test_H,Xval_org, Xval_org_mask, val_input, val_H )\n",
    "        \n",
    "\n",
    "        train_with_early_stopping(imputer, train_loader, test_loader, epoch=10, patience=10)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            imputer.eval()\n",
    "            RMSE_total = []\n",
    "            for truth_X, mask, data_X, x_hat in test_loader:\n",
    "\n",
    "                RMSE, prediction =  loss(truth=truth_X, mask=mask, data=data_X,imputer = imputer)\n",
    "                imputed_data = impute_with_prediction(truth_X, mask, prediction)\n",
    "                RMSE_total.append(RMSE)\n",
    "\n",
    "\n",
    "        RMSE_tensor = torch.tensor(RMSE_total)\n",
    "        rmse_final = torch.mean(RMSE_tensor)\n",
    "\n",
    "        Imputer_RMSE.append(round(rmse_final.item(),5))\n",
    "\n",
    "        print('Final Test RMSE: {:.4f}'.format(rmse_final.item()))\n",
    "\n",
    "\n",
    "    print([rule_name for rule_name in missing_rule])\n",
    "    print(Imputer_RMSE,baseline_RMSE)\n",
    "\n",
    "    result = pd.DataFrame({\"Missing_Rule\":[rule_name for rule_name in missing_rule],\"Imputer RMSE\":Imputer_RMSE,\"Baseline RMSE\":baseline_RMSE})\n",
    "    print(Imputer_RMSE)\n",
    "    result.to_csv(\"results/GAIN_imputer/{}_0pass.csv\".format(dataset_file),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "california C0_lower False [True, True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:06,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain_loss: 0.3041\tVal_loss: 0.1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:01<00:05,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTrain_loss: 0.09684\tVal_loss: 0.0898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:02<00:05,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\tTrain_loss: nan\tVal_loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:02<00:04,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\tTrain_loss: nan\tVal_loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:03<00:03,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\tTrain_loss: nan\tVal_loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:04<00:02,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\tTrain_loss: nan\tVal_loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:05<00:02,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\tTrain_loss: nan\tVal_loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:05<00:01,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\tTrain_loss: nan\tVal_loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:06<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\tTrain_loss: nan\tVal_loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\tTrain_loss: nan\tVal_loss: nan\n",
      "Final Test RMSE: nan\n",
      "['C0_lower']\n",
      "[nan] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run2(dataset_file,missing_rule,use_BN)\n",
      "Cell \u001b[1;32mIn[24], line 48\u001b[0m, in \u001b[0;36mrun2\u001b[1;34m(dataset_file, missing_rule, use_BN)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mprint\u001b[39m([rule_name \u001b[39mfor\u001b[39;00m rule_name \u001b[39min\u001b[39;00m missing_rule])\n\u001b[0;32m     46\u001b[0m \u001b[39mprint\u001b[39m(Imputer_RMSE,baseline_RMSE)\n\u001b[1;32m---> 48\u001b[0m result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m\"\u001b[39;49m\u001b[39mMissing_Rule\u001b[39;49m\u001b[39m\"\u001b[39;49m:[rule_name \u001b[39mfor\u001b[39;49;00m rule_name \u001b[39min\u001b[39;49;00m missing_rule],\u001b[39m\"\u001b[39;49m\u001b[39mImputer RMSE\u001b[39;49m\u001b[39m\"\u001b[39;49m:Imputer_RMSE,\u001b[39m\"\u001b[39;49m\u001b[39mBaseline RMSE\u001b[39;49m\u001b[39m\"\u001b[39;49m:baseline_RMSE})\n\u001b[0;32m     49\u001b[0m \u001b[39mprint\u001b[39m(Imputer_RMSE)\n\u001b[0;32m     50\u001b[0m result\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mresults/GAIN_imputer/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_0pass.csv\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(dataset_file),index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32me:\\ANACONDA\\envs\\py3.10\\lib\\site-packages\\pandas\\core\\frame.py:663\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    657\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    658\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    659\u001b[0m     )\n\u001b[0;32m    661\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    662\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 663\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    664\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    665\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32me:\\ANACONDA\\envs\\py3.10\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32me:\\ANACONDA\\envs\\py3.10\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32me:\\ANACONDA\\envs\\py3.10\\lib\\site-packages\\pandas\\core\\internals\\construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[0;32m    665\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    668\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[0;32m    669\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    670\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "run2(dataset_file,missing_rule,use_BN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
